<!doctype html>
<html lang="fr">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <title>Générateur automatique de projets dbt — Raphael DIEZ PECOSTE</title>

    <!-- Feuilles de style (conservées séparées comme convenu) -->
    <link rel="stylesheet" href="../css/variables.css" />
    <link rel="stylesheet" href="../css/base.css" />
    <link rel="stylesheet" href="../css/header-footer.css" />
    <link rel="stylesheet" href="../css/layout.css" />
    <link rel="stylesheet" href="../css/components.css" />
    <link rel="stylesheet" href="../css/project-details.css" />
    <link rel="stylesheet" href="../css/file_tree.css" />
    <link rel="stylesheet" href="../css/responsive.css" />
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css"
    />

    <meta
      name="description"
      content="Générateur automatique de projets dbt - Page descriptive du projet (LangGraph, Groq, Snowflake, Streamlit)"
    />
  </head>
  <body>
    <!-- Header / breadcrumb peupler via votre JS existant -->
    <div id="header"></div>
    <div id="breadcrumb"></div>
    <script src="../js/breadcrumb.js" defer></script>

    <!-- Contenu principal -->
    <main class="project-detail">
      <div class="container" style="color: blue">
        <!-- Hero 
      <header class="hero project-hero" >
        <div >
          <div >
            <h1 >
              Générateur automatique de projets <strong>dbt</strong>
            </h1>
            <p >
              Outil qui transforme une spécification métier (XLSX/CSV) et une description en langage naturel
              en un projet DBT complet et prêt à l'emploi. Orchestration via <strong>LangGraph</strong>, LLM via
              <strong>Groq</strong> et intégration Snowflake.
            </p>

            <div class="cta-buttons">
              <a class="btn-primary" href="streamlit:run app.py" >Lancer Streamlit</a>
              <a class="btn-secondary" href="../docs/Documentation%20(4).pdf" download> Télécharger la documentation (PDF)</a>
            </div>
          </div>

          <div >
            <figure >
              <img src="../assets/images/dbt-generator-streamlit.png"
                   alt="Capture interface Streamlit"
                    />
              <figcaption >Interface Streamlit — suivi en temps réel</figcaption>
            </figure>
          </div>
        </div>
      </header>
      -->
        <!-- Bloc description -->
        <section class="project-description timeline-item">
          <h2>Description synthétique</h2>
          <div class="project-description__body">
            <p>
              Le générateur automatise la création d'un projet DBT (staging →
              intermediate → marts) à partir d’un fichier de spécification et
              d’une consigne en langage naturel. Le cœur du pipeline est un
              graphe LangGraph dont chaque nœud exécute une action sur un état
              partagé (state). L’étape pivot est la génération d’un
              <em>plan</em> structuré par un LLM (Groq), ensuite converti en
              fichiers SQL et YAML.
            </p>
          </div>
        </section>

        <!-- Technical stack -->
        <section class="project-technical-stack timeline-item">
          <div>
            <h3>Stack technique</h3>
            <ul>
              <li>Python 3.12+, dbt-core</li>
              <li>LangGraph (orchestration)</li>
              <li>Groq (LLM) — modèle configurable via <code>.env</code></li>
              <li>Snowflake (ingestion des métadonnées)</li>
              <li>Streamlit (interface web)</li>
            </ul>
          </div>

          <div>
            <h3>Fichiers clefs</h3>
            <p>
              Le projet est organisé de manière modulaire :
              <code>src/core</code> (logique), <code>src/graph</code> (nœuds &
              arêtes), <code>formats/prompts</code> (templates jinja2),
              <code>inputs/</code> et <code>outputs/</code>.
            </p>
            <a class="btn-secondary" href="../README.md">Voir README complet</a>
          </div>
        </section>

        <section class="project-architecture timeline-item">
          <h2>Architecture interne du projet</h2>
          <p>
            Le générateur s’appuie sur une architecture modulaire en couches :
            la CLI (<code>main.py</code>), les <strong>tasks</strong> pour les
            actions de haut niveau (install, run, clean...), le
            <strong>graphe LangGraph</strong> pour l’orchestration logique, et
            des modules <strong>core</strong> pour les fonctions métier
            (Snowflake, LLM, DBT).
          </p>
          <figure>
            <img
              src="../docs/Architecture_schema.png"
              alt="Architecture interne du projet"
            />
            <figcaption>
              Schéma de l’architecture interne (modules, graphes et dépendances)
            </figcaption>
          </figure>

          <!-- Explorateur de fichiers -->
          <h3>Arborescence du projet</h3>
          <div id="file-tree"></div>

          <!-- Scripts -->
          <script src="../js/file_tree.js"></script>
          <script>
            (function () {
              const url = "../assets/project-content/dbt-generator/tree.json";
              fetch(url)
                .then((res) => {
                  if (!res.ok) throw new Error(`HTTP ${res.status} on ${url}`);
                  return res.json();
                })
                .then((data) => renderFileTree("file-tree", data))
                .catch((err) => {
                  console.error("[file_tree] Échec du chargement JSON:", err);
                  const container = document.getElementById("file-tree");
                  if (container) {
                    container.textContent =
                      "Impossible de charger l’arborescence.";
                  }
                });
            })();
          </script>
        </section>

        <section class="project-graph-structure timeline-item">
          <h2>Structure du graphe LangGraph</h2>
          <p>
            L’un des aspects clés du projet est l’orchestration asynchrone des
            étapes via <strong>LangGraph</strong>. Chaque nœud du graphe exécute
            une fonction spécifique et modifie un <code>State</code> partagé
            entre tous les nœuds. Cela permet de suivre l’évolution du contexte
            global et d’assurer la continuité du pipeline, même entre plusieurs
            itérations.
          </p>
          <figure>
            <img
              src="../docs/graphViz.drawio.png"
              alt="Structure du graphe LangGraph"
            />
            <figcaption>Vue simplifiée du graphe LangGraph</figcaption>
          </figure>
          <p>
            Chaque nœud représente une étape clé du pipeline : ingestion,
            planification, génération, sauvegarde, ou fin de processus. Les
            arêtes conditionnelles (<em>edges</em>) contrôlent la logique selon
            la présence d’un plan ou de fichiers précédemment sauvegardés.
          </p>

          <h3>Structure du State</h3>
          <pre>
                from typing import List, Dict, Any
                from typing_extensions import TypedDict


                class State(TypedDict, total=False):
                    """
                    Represents the shared state across different steps of the generation process.
                    """

                    plan: dict
                    generation_status: list = []
                    generated_files: dict
                    expected_format: dict
                    file_specs: dict
                    spec_file: str
                    ingested_marts_data: Dict[str, Any] = []
                    sources: dict
                    use_previous_plan: bool
                    path_to_saved_plan: str
                    use_previous_files: bool
                    path_to_saved_files: str
                    save_project: bool
                    project_name: str
                    profile_name: str
                    base_path: str
                    update_graph_status: str
          </pre>

          <p>
            Cette structure normalisée permet de centraliser toutes les données
            de contexte (spécification, plan, fichiers générés, etc.) et de
            passer proprement les informations entre nœuds. Cela facilite aussi
            la sauvegarde intermédiaire pour relancer le pipeline à mi-parcours
            si besoin.
          </p>
        </section>

        <!-- Workflow / Graph -->
        <section class="project-workflow timeline-item">
          <h2>Flux d'exécution (vue globale)</h2>

          <!-- Visual placeholder (SVG) pour le graphe LangGraph -->
          <!-- Insérer capture d'écran -->
        </section>

        <!-- Snowflake ingestion details -->
        <section class="project-snowflake-params timeline-item">
          <h2>Ingestion des métadonnées — Snowflake</h2>
          <p>
            Le pipeline peut connecter Snowflake pour extraire la liste des
            tables et des colonnes (nom, type data). Ces métadonnées sont
            injectées dans <code>state["expected_format"]["sources"]</code> et
            utilisées pour enrichir le prompt du LLM et produire un
            <code>schema.yml</code> conforme.
          </p>

          <div>
            <div>
              <h4>Paramètres Snowflake requis</h4>
              <table>
                <tr>
                  <th>SNOWFLAKE_ACCOUNT</th>
                  <td>
                    Identifiant du compte (ex. <code>xy12345.eu-west-1</code>)
                  </td>
                </tr>
                <tr>
                  <th>SNOWFLAKE_USER</th>
                  <td>Utilisateur DBT</td>
                </tr>
                <tr>
                  <th>SNOWFLAKE_PASSWORD</th>
                  <td>Mot de passe (masqué dans l'UI)</td>
                </tr>
                <tr>
                  <th>SNOWFLAKE_ROLE</th>
                  <td>Rôle (ex. <code>SYSADMIN</code>)</td>
                </tr>
                <tr>
                  <th>SNOWFLAKE_WAREHOUSE</th>
                  <td>Entrepôt (compute)</td>
                </tr>
                <tr>
                  <th>SNOWFLAKE_DATABASE / SCHEMA</th>
                  <td>
                    Base et schéma cibles (ex. <code>ANALYTICS_DB.PUBLIC</code>)
                  </td>
                </tr>
              </table>
            </div>

            <div>
              <h4>Comportement</h4>
              <ul>
                <li>
                  La connexion s’effectue via la librairie Snowflake Connector
                  (config dans <code>config/variables.py</code>).
                </li>
                <li>
                  Le noeud <code>node_ingestion</code> appelle
                  <code>ingest_metadata_from_snowflake_to_sources_format</code>.
                </li>
                <li>
                  Les métadonnées retournées sont normalisées sous la forme
                  attendue par DBT (liste de sources → tables → colonnes).
                </li>
              </ul>
            </div>
          </div>
        </section>

        <!-- =========================
      CLI & Tasks (install, run, clean, backup)
      ========================= -->
        <section class="project-cli-tasks timeline-item">
          <h2>CLI & Tasks — Utilisation sans interface graphique</h2>
          <p>
            Le projet propose une interface en ligne de commande (entrée
            principale : <code>main.py</code>) qui délègue des actions « tâches
            » (install, init, run, clean, backup) implémentées sous
            <code>src/core/tasks/</code>. Cela permet d'exécuter l'ensemble du
            pipeline dans un environnement headless (serveur CI, VM, etc.).
          </p>

          <h3>Commandes principales</h3>
          <ul>
            <li>
              <code>python main.py install</code> — crée un environnement
              virtuel et installe <code>requirements.txt</code>.
            </li>
            <li>
              <code>python main.py init</code> — lance la configuration
              interactive (.env, profiles.yml) via popups Tkinter.
            </li>
            <li>
              <code>python main.py run</code> — exécution interactive du
              pipeline en console (live logs).
            </li>
            <li>
              <code>python main.py clean</code> — supprime
              <code>__pycache__</code> et <code>outputs/</code>.
            </li>
            <li>
              <code>python main.py backup &lt;project_name&gt;</code> — crée un
              snapshot horodaté dans <code>backups/</code>.
            </li>
          </ul>

          <h3>Extrait : squelette d’un task (ex: task_run)</h3>
          <pre>
      # src/core/tasks/task_run.py (schéma)
      def run_console():
          base_path = input("Base path: ")
          project_name = input("Project name: ")
          spec_file = open(input("Path to specs: "), "rb")
          # configure logger (InMemoryLogger)
          reset_graph_status()
          result_state = generate_dbt_project(..., spec_file, ...)
          # stream logs while thread alive
          # print summary
        </pre
          >

          <figure>
            <!-- Insérer : diagramme séquentiel montrant main.py -> tasks -> generate_dbt_project -->
            <img
              src="../docs/cli_tasks_sequence.png"
              alt="Séquence CLI → Tasks → pipeline"
            />
            <figcaption>
              Flux d’appel : CLI → tâche → orchestration (generate_dbt_project).
            </figcaption>
          </figure>
        </section>

        <!-- =========================
     Noeuds détaillés (init, ingestion, plan, generate_source_yaml, prepare_file_specs, generate_file, create_file, save_outputs, end_process)
     ========================= -->
        <section class="project-nodes-detailed timeline-item">
          <h2>Descriptions détaillées des nœuds du graphe</h2>
          <p>
            Ci-dessous la liste des nœuds principaux avec leur rôle, entrées,
            traitement et sorties. Ces descriptions correspondent aux fichiers
            dans <code>src/graph/nodes/</code>.
          </p>

          <!-- node_init_project -->
          <article class="node" id="node-init-project">
            <h3>node_init_project — Initialisation du projet</h3>
            <p>
              Rôle : préparer l’arborescence du projet DBT (dossiers models/,
              tests/, marts/, logs/, saved/), injecter les paramètres initiaux
              (project_name, base_path, profile_name) dans <code>state</code> et
              vérifier les conflits de nom.
            </p>
            <ul>
              <li>
                <strong>Entrées</strong> :
                <code>project_name, base_path, profile_name, spec_file</code>
              </li>
              <li>
                <strong>Traitement</strong> : vérifie existence du dossier,
                incrémente suffixe si besoin, crée répertoires.
              </li>
              <li>
                <strong>Sorties</strong> :
                <code
                  >state.base_path, state.project_name, state.path_to_logs</code
                >.
              </li>
            </ul>
          </article>

          <!-- node_ingestion -->
          <article class="node" id="node-ingestion">
            <h3>
              node_ingestion — Ingestion des spécifications & métadonnées
              Snowflake
            </h3>
            <p>
              Rôle : lire le fichier utilisateur (XLSX/CSV/TSV) pour produire
              <code>ingested_marts_data</code> et appeler la fonction Snowflake
              pour récupérer la liste des tables/colonnes et alimenter
              <code>expected_format["sources"]</code>.
            </p>
            <ul>
              <li>
                <strong>Entrées</strong> :
                <code>state.spec_file</code> (UploadedFile ou file handle),
                connexion Snowflake via config.
              </li>
              <li>
                <strong>Traitement</strong> : parsing de l’excel →
                standardisation (column_name, description, sql) ; appel à
                <code
                  >ingest_metadata_from_snowflake_to_sources_format(log)</code
                >.
              </li>
              <li>
                <strong>Sorties</strong> :
                <code
                  >state.ingested_marts_data,
                  state.expected_format["sources"]</code
                >.
              </li>
            </ul>
            <figure>
              <img
                src="../docs/snowflake_metadata_flow.png"
                alt="Flow ingestion Snowflake"
              />
              <figcaption>
                Exemple d’export de métadonnées Snowflake (tables → colonnes →
                types).
              </figcaption>
            </figure>
          </article>

          <!-- node_generate_plan -->
          <article class="node" id="node-generate-plan">
            <h3>node_generate_plan — Génération d’un plan structuré par LLM</h3>
            <p>
              Rôle : construire un prompt enrichi (Jinja2 template) contenant
              <code>expected_format</code> et <code>ingested_marts_data</code>,
              appeler le LLM (via <code>get_llm()</code>) et parser la réponse
              JSON en <code>state.plan</code>.
            </p>
            <ul>
              <li>
                <strong>Entrées</strong> :
                <code>state.expected_format, state.ingested_marts_data</code>
              </li>
              <li>
                <strong>Traitement</strong> : rendu du prompt (PLAN_PROMPT_PATH)
                → appel LLM → <code>extract_plan()</code> → validation →
                stockage.
              </li>
              <li>
                <strong>Sorties</strong> : <code>state.plan</code> (structures :
                models[], tests[], dependencies[]).
              </li>
            </ul>
            <p>
              Note : le module gère les erreurs LLM (timeout, rate-limit, valeur
              non-json) et incrémente un compteur de retry.
            </p>
            <p>
              [insérer exemple : plan produit complet — voir
              <code>outputs/&lt;project&gt;/saved/saved_plan.json</code>]
            </p>
          </article>

          <!-- node_generate_source_yaml -->
          <article class="node" id="node-generate-source-yaml">
            <h3>
              node_generate_source_yaml — Génération de
              <code>schema.yml</code> pour les sources
            </h3>
            <p>
              Rôle : à partir de <code>expected_format["sources"]</code> et
              <code>state.plan["models"]</code>, produire un YAML compatible DBT
              (version 2) décrivant sources + models.
            </p>
            <ul>
              <li>
                <strong>Entrées</strong> :
                <code>state.expected_format["sources"], state.plan</code>
              </li>
              <li>
                <strong>Traitement</strong> : construction des blocs
                <code>sources</code> et <code>models</code>, heuristiques (tests
                not_null si nom de colonne se termine par "id").
              </li>
              <li>
                <strong>Sorties</strong> :
                <code>state.generated_files["sources"][...]</code> et fichier
                écrit sous <code>models/staging/schema.yml</code>.
              </li>
            </ul>
          </article>

          <!-- node_prepare_file_specs -->
          <article class="node" id="node-prepare-file-specs">
            <h3>
              node_prepare_file_specs — Préparation des spécifications de
              fichiers
            </h3>
            <p>
              Rôle : transformer le <code>state.plan</code> en spécifications
              complètes pour chaque fichier attendu (models, tests). Chaque spec
              contient fields requis (name, description, path, columns,
              dependencies, filetype).
            </p>
            <pre>
# Extrait d'un spec (simplifié)
"models": {
  "models/staging/stg_expeditions": {
    "name": "stg_expeditions",
    "path": "models/staging/",
    "columns": [...],
    "dependencies": "himalayan_expeditions_seeds.EXPED",
    "filetype": "models"
  }, ...
}
    </pre
            >
          </article>

          <!-- node_generate_file -->
          <article class="node" id="node-generate-file">
            <h3>node_generate_file — Génération de code via LLM (templates)</h3>
            <p>
              Rôle : pour chaque spec, construire un contexte (Jinja2) et
              appeler le LLM pour produire le contenu du fichier (SQL pour
              models/tests). Les prompts sont stockés sous
              <code>formats/prompts/</code> (e.g.,
              <code>prompt_file_model.txt</code>).
            </p>
            <ul>
              <li>
                <strong>Entrées</strong> :
                <code>state.file_specs[filetype]</code>
              </li>
              <li>
                <strong>Traitement</strong> : rendu du prompt →
                LLM.invoke(prompt) → extraire
                <code>response.content</code> (cleanup code fences).
              </li>
              <li>
                <strong>Sorties</strong> :
                <code>state.generated_files[filetype][path] = content</code>.
              </li>
            </ul>
          </article>

          <!-- node_create_file -->
          <article class="node" id="node-create-file">
            <h3>node_create_file — Écriture des fichiers sur disque</h3>
            <p>
              Rôle : écrire physiquement les fichiers DBT à partir de
              <code>generated_files</code>, en nettoyant le contenu et en
              respectant l’architecture DBT.
            </p>
            <ul>
              <li>
                <strong>Entrées</strong> :
                <code
                  >state.generated_files, state.file_specs,
                  state.base_path</code
                >
              </li>
              <li>
                <strong>Traitement</strong> : extraire bloc de code, composer
                chemin complet, créer répertoires, écrire via
                <code>write_code_to_file()</code>.
              </li>
              <li>
                <strong>Sorties</strong> : fichiers écrits et logs d’écriture.
              </li>
            </ul>
          </article>

          <!-- node_save_outputs -->
          <article class="node" id="node-save-outputs">
            <h3>node_save_outputs — Persist & save</h3>
            <p>
              Rôle : si l’utilisateur a demandé la sauvegarde, écrire
              <code>saved/saved_plan.json</code> et
              <code>saved/saved_files.json</code>
              dans le dossier du projet pour permettre réutilisation / débogage
              ultérieur.
            </p>
            <ul>
              <li>
                <strong>Entrées</strong> :
                <code>state.plan, state.generated_files, state.base_path</code>
              </li>
              <li>
                <strong>Sortie</strong> : fichiers JSON sous
                <code>&lt;base_path&gt;/saved/</code>.
              </li>
            </ul>
          </article>

          <!-- node_end_process & node_end_process_bad_plan_generation -->
          <article class="node" id="node-end-process">
            <h3>
              node_end_process &amp; node_end_process_bad_plan_generation — Fin
              du pipeline
            </h3>
            <p>
              Le nœud final nettoie l’état (serialisation sûre via
              <code>safe_serialize</code>), construit un résumé lisible
              (<code>state.result_summary</code>) et s’assure que l’état peut
              être dumpé (JSON). Un nœud spécifique gère le cas d’un échec
              récurrent de génération de plan (bad_plan_generation).
            </p>
            <ul>
              <li>
                <strong>Comportement</strong> : compile un résumé, transforme
                les objets non-serialisables en chaîne, log final.
              </li>
            </ul>
          </article>
        </section>

        <!-- =========================
     Logging & monitoring
     ========================= -->
        <section class="project-logging timeline-item">
          <h2>Logging & Monitoring</h2>
          <p>
            Le projet implémente un logger en mémoire (affiché dans Streamlit)
            et des handlers vers des fichiers: <code>debug.log</code>,
            <code>warnings.log</code>, <code>errors.log</code>. Le module
            <code>InMemoryLogger</code> encapsule ces comportements (formatters,
            filters, redirection de stderr).
          </p>

          <h3>Comportement clé</h3>
          <ul>
            <li>
              Logs affichés en temps réel dans Streamlit via la fonction
              <code>display_graph_and_logs_placeholders()</code>.
            </li>
            <li>
              Buffer en mémoire (StringIO) pour téléchargement / affichage
              final.
            </li>
            <li>
              Handlers : INFO→info.log, DEBUG→debug.log, WARNING→warnings.log
              (exact), ERROR→errors.log (exact).
            </li>
          </ul>

          <figure>
            <img
              src="../docs/logging_architecture.png"
              alt="Architecture logging"
            />
            <figcaption>
              Schéma : flux des logs (Worker thread → InMemoryLogger → Streamlit
              UI et fichiers).
            </figcaption>
          </figure>

          <h3>Conseil pratique</h3>
          <p>
            En environnement de production, envisager la redirection des
            fichiers de logs vers un stockage centralisé (ELK, CloudWatch) et
            augmenter la rotation des fichiers.
          </p>
        </section>

        <!-- =========================
     Backups, outputs et archive
     ========================= -->
        <section class="project-backups timeline-item">
          <h2>Backups & outputs</h2>
          <p>
            Le dossier <code>outputs/</code> contient les projets générés ; la
            commande <code>backup</code> copie un snapshot horodaté dans
            <code>backups/</code>. Les sauvegardes incluent les logs et le
            dossier <code>saved/</code> (saved_plan.json, saved_files.json).
          </p>

          <h3>Exemple d’arborescence sauvegardée</h3>
          <pre>
outputs/
└─ my_project/
   ├─ models/
   ├─ tests/
   ├─ logs/
   │   ├─ debug.log
   │   └─ generation.log
   └─ saved/
       ├─ saved_plan.json
       └─ saved_files.json
  </pre
          >

          <p>
            Pour restaurer, il suffit de copier le dossier
            (<code>backups/my_project_&lt;timestamp&gt;</code>) vers
            <code>outputs/</code> ou d’extraire
            <code>saved_plan.json</code> &amp; <code>saved_files.json</code>
            pour relancer le pipeline en mode « reuse previous plan/files ».
          </p>
        </section>

        <!-- =========================
     Sécurité & gestion des identifiants (Snowflake, Groq)
     ========================= -->
        <section class="project-security timeline-item">
          <h2>Sécurité & gestion des identifiants</h2>
          <p>
            Les secrets (GROQ_API_KEY, SNOWFLAKE_PASSWORD, etc.) sont stockés
            dans <code>.env</code>. L’outil interactif (<code>init</code>)
            propose une saisie via fenêtres masquées (Tkinter). Ne commitez
            jamais <code>.env</code> dans Git.
          </p>

          <h3>Bonnes pratiques</h3>
          <ul>
            <li>
              Utiliser des comptes de service dédiés avec des rôles limités
              (minimum nécessaire).
            </li>
            <li>
              Ne donner que les droits nécessaires au profil DBT (lectures
              métadonnées / selects si besoin).
            </li>
            <li>
              Limiter la rotation d’API keys et mettre en place un stockage
              secret (Vault, GitHub Secrets) en production.
            </li>
          </ul>

          <h3>Détails Snowflake</h3>
          <p>
            Les paramètres à renseigner sont : <code>SNOWFLAKE_ACCOUNT</code>,
            <code>SNOWFLAKE_USER</code>, <code>SNOWFLAKE_PASSWORD</code>,
            <code>SNOWFLAKE_ROLE</code>, <code>SNOWFLAKE_WAREHOUSE</code>,
            <code>SNOWFLAKE_DATABASE</code>, <code>SNOWFLAKE_SCHEMA</code>. Le
            rôle doit avoir le droit d’exécuter <code>SHOW TABLES</code> et
            <code>SHOW COLUMNS</code> dans le schéma ciblé.
          </p>
        </section>

        <!-- =========================
     Extensibilité & bonnes pratiques
     ========================= -->
        <section class="project-extensibility timeline-item">
          <h2>Extensibilité & bonnes pratiques</h2>
          <p>
            Le code est conçu pour être modulaire. Voici des axes d’amélioration
            faciles à implémenter :
          </p>
          <ul>
            <li>
              <strong>Multi-marts</strong> : supporter plusieurs marts dans le
              même plan (boucler sur plusieurs dossiers / domaines).
            </li>
            <li>
              <strong>Fichiers dépendants</strong> : intégrer la génération
              d’assets SQL partagés et la résolution automatique des
              dépendances.
            </li>
            <li>
              <strong>Cache LLM</strong> : stocker les requêtes/réponses LLM
              pour éviter des coûts et accélérer les essais.
            </li>
            <li>
              <strong>Tests unitaires</strong> : ajouter validations sur
              <code>plan</code> (schéma JSON) et fixtures pour les prompts.
            </li>
          </ul>
          <p>[insérer suggestions graphiques / diagrammes d’extension]</p>
        </section>

        <!-- Example plan -->
        <section class="project-plan-example timeline-item">
          <h2>Exemple (extrait) de plan produit</h2>

          <p>
            Ci-dessous un extrait abrégé du JSON produit par le LLM et stocké
            dans <code>state["plan"]</code> :
          </p>

          <pre>
{
  "models": [
    {
      "name": "stg_expeditions",
      "description": "A staging model for the EXPED table",
      "model_type": "stage",
      "domain": "himalayan_expeditions",
      "path": "models/staging/stg_expeditions.sql",
      "columns": [
        {"name": "expid", "data_type": "TEXT"},
        {"name": "year", "data_type": "FIXED"},
        ...
      ],
      "tests": [
        {"name": "not_null_expid", "description": "expid should not be null"}
      ]
    },
    {
      "name": "int_expeditions_with_peaks",
      "description": "Join of expeditions + peaks",
      "model_type": "intermediate",
      "dependencies": ["stg_expeditions", "stg_peaks"],
      ...
    }
  ],
  "tests": [
    {"name": "test_stg_expeditions_not_null", "model": "stg_expeditions", ...}
  ]
}
        </pre
          >

          <p>
            [insérer exemple complet : plan produit] — le plan réel sauvegardé
            est plus verbeux (sources, tables, colonnes, chemins).
          </p>
        </section>

        <!-- Gallery & Downloads -->
        <section class="project-gallery timeline-item">
          <h2>Visuels & documentation</h2>

          <div>
            <figure>
              <img src="../docs/graphViz.drawio.png" alt="Graphe LangGraph" />
              <figcaption>
                Schéma du graphe LangGraph – orchestration du pipeline IA
              </figcaption>
            </figure>

            <figure>
              <img
                src="../docs/Orchestrenom.drawio.png"
                alt="Architecture du pipeline complet"
              />
              <figcaption>
                Architecture globale : modules, nœuds et connexions du système
              </figcaption>
            </figure>

            <figure>
              <img
                src="../docs/exemple_specs.png"
                alt="Exemple de spécification Excel"
              />
              <figcaption>
                Exemple de fichier de spécification Excel utilisé pour
                l’ingestion
              </figcaption>
            </figure>
          </div>

          <div>
            <a
              href="../docs/Documentation%20(4).pdf"
              class="btn-primary"
              download
            >
              <i class="fa-solid fa-file-pdf"></i>
              Télécharger la documentation complète (PDF)
            </a>
          </div>
        </section>

        <section class="project-streamlit timeline-item">
          <h2>Interface Streamlit</h2>
          <p>
            L’interface graphique permet de lancer la génération DBT sans passer
            par la CLI. Elle affiche les logs en temps réel, une animation
            d’attente, et propose un accès direct au projet généré dans le
            répertoire <code>outputs/</code>.
          </p>
          <figure>
            <img
              src="../assets/images/dbt-generator-streamlit.png"
              alt="Interface Streamlit du projet"
            />
            <figcaption>
              Interface Streamlit — génération et suivi en direct
            </figcaption>
          </figure>
        </section>

        <!-- Learnings -->
        <section class="project-learnings timeline-item">
          <h2>Compétences & enseignements</h2>
          <p>
            Ce projet m’a permis d’approfondir la conception d’architectures de
            données intelligentes et la génération de code automatisée. J’ai
            exploré en profondeur :
          </p>
          <ul>
            <li>
              L’orchestration de workflows complexes avec
              <strong>LangGraph</strong>.
            </li>
            <li>
              L’intégration et la configuration avancée de
              <strong>Snowflake</strong>.
            </li>
            <li>
              La génération dynamique de fichiers <strong>dbt</strong> (modèles,
              tests, <code>schema.yml</code>).
            </li>
            <li>
              La communication et le prompt engineering pour un
              <strong>LLM Groq</strong>.
            </li>
            <li>
              La création d’une interface <strong>Streamlit</strong> réactive
              avec logs en temps réel.
            </li>
          </ul>
          <p>
            Au-delà de la technique, ce projet démontre comment l’IA peut
            accélérer le développement de pipelines de données robustes et
            documentés.
          </p>
        </section>

        <!-- Links -->
        <section class="project-links timeline-item">
          <h2>Liens utiles</h2>
          <ul>
            <li>
              <a
                href="https://github.com/username/dbt-project-generator"
                target="_blank"
                ><i class="fa-brands fa-github"></i> Code source sur GitHub</a
              >
            </li>
            <li>
              <a href="https://docs.getdbt.com/" target="_blank"
                ><i class="fa-solid fa-book"></i> Documentation officielle
                dbt</a
              >
            </li>
            <li>
              <a
                href="https://console.groq.com/docs/rate-limits"
                target="_blank"
                ><i class="fa-solid fa-brain"></i> Documentation Groq</a
              >
            </li>
            <li>
              <a href="https://www.snowflake.com/en/" target="_blank"
                ><i class="fa-solid fa-snowflake"></i> Snowflake Cloud Data
                Platform</a
              >
            </li>
          </ul>
        </section>
      </div>
    </main>

    <!-- Footer -->
    <div id="footer"></div>
    <script src="../js/dynamic-content.js" defer></script>
  </body>
</html>
